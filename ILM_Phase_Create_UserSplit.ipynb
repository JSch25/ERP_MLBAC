{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25688f8-ca11-4223-89d8-f7c5b8b4e1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037b0977-96b0-43eb-b2c1-8a2f1c5dd594",
   "metadata": {},
   "source": [
    "Replace the placeholder with the correct filename. The data is available as a CSV file in the properly preprocessed format. The dataset consists of the attributes 'USER' and 'TCODE' (transaction permission), along with the respective labels for UPA_I, UPA_T, and UPA_R. In addition, the six user attributes ('COMPANY', 'CLASS', 'KOSTL', 'USTYP', 'DEPARTMENT', 'FUNCTION') and the permission attribute 'SUBCOMPONENT' are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf2211f-e1b3-4fa5-9d60-bad4151947f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlbac_data = pd.read_csv('mlbac_data.csv',low_memory=False).sample(frac=1, random_state=42).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35de0a02-8e4c-4bda-8f28-0a69fdc51d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features_all_attributes = ['USER','TCODE','COMPANY', 'CLASS', 'KOSTL', 'USTYP', 'DEPARTMENT', 'FUNCTION', 'SUBCOMPONENT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f749f96a-02ff-4b41-9715-7ce7ca0a7162",
   "metadata": {},
   "outputs": [],
   "source": [
    "UPA_I = mlbac_data.drop('Target_UPA_T', axis=1).rename(columns={'Target_UPA_I': 'Target'})\n",
    "\n",
    "UPA_T = mlbac_data.loc[(mlbac_data[\"Target_UPA_T\"]==1)|(mlbac_data[\"Target_UPA_T\"]==0)].drop('Target_UPA_I', axis=1).rename(columns={'Target_UPA_T': 'Target'})\n",
    "\n",
    "UPA_R = mlbac_data.drop('Target_UPA_I', axis=1).rename(columns={'Target_UPA_T': 'Target'})\n",
    "UPA_R = UPA_R.replace(2,0, regex=True)\n",
    "\n",
    "datasets_dict = {\n",
    "    'UPA_I': (UPA_I, cat_features_all_attributes),\n",
    "    'UPA_T': (UPA_T, cat_features_all_attributes),\n",
    "    'UPA_R': (UPA_R, cat_features_all_attributes)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299ea688-9f7f-40ad-ad6e-28e08b7d9eca",
   "metadata": {},
   "source": [
    "## UserSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29385c31-66c2-4e12-9eaa-5eb6668a49d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_split_data(X_train, X_val, X_test, y_train, y_val, y_test, y_test_pred, y_test_pred_proba, tn, fp, fn, tp, dataset_name, run):\n",
    "    # Mark training data\n",
    "    train_data = X_train.copy()\n",
    "    train_data['True_Target'] = y_train\n",
    "    train_data['Predicted_Target'] = None  # Keine Vorhersagen für Trainingsdaten\n",
    "    train_data['Split'] = 'Train'\n",
    "    \n",
    "    # Mark validation data\n",
    "    val_data = X_val.copy()\n",
    "    val_data['True_Target'] = y_val\n",
    "    val_data['Predicted_Target'] = None  # Keine Vorhersagen für Validierungsdaten\n",
    "    val_data['Split'] = 'Validation'\n",
    "    \n",
    "    # Mark test data\n",
    "    test_data = X_test.copy()\n",
    "    test_data['True_Target'] = y_test\n",
    "    test_data['Predicted_Target'] = y_test_pred\n",
    "    test_data['Split'] = 'Test'\n",
    "    \n",
    "    # Combine all splits into one DataFrame\n",
    "    combined_data = pd.concat([train_data, val_data, test_data], axis=0, ignore_index=True)\n",
    "    \n",
    "    # Define the file name\n",
    "    file_name = f'{dataset_name}_run_{run+1}_Usersplit.csv'\n",
    "    \n",
    "    # Save the combined data\n",
    "    combined_data.to_csv(file_name, index=False)\n",
    "    print(f'Dataset saved as {file_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823e6c16-b2d5-4be2-a60c-5a1fe2fa3aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(datasets_dict, num_runs, train_size, test_size, use_custom_split):\n",
    "    results = [] \n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        random_state = run * 10\n",
    "\n",
    "        users_train, users_val, users_test = None, None, None\n",
    "\n",
    "        for dataset_name, (dataset, cat_features) in datasets_dict.items():\n",
    "            print(f\"\\nDataset: {dataset_name} | Run: {run + 1} | Random State: {random_state} | Number of Data Points: {len(dataset)}\")\n",
    "\n",
    "            X = dataset.drop(\"Target\", axis=1)\n",
    "            y = dataset[\"Target\"]\n",
    "    \n",
    "            split_method = 'Custom Split' if use_custom_split else 'Standard Split'\n",
    "    \n",
    "            if use_custom_split:\n",
    "                unique_users = X['USER'].unique()\n",
    "\n",
    "                if users_train is None:\n",
    "                    users_train, users_temp = train_test_split(unique_users, test_size=1-train_size, random_state=random_state)\n",
    "                    users_val, users_test = train_test_split(users_temp, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "                X_train = X[X['USER'].isin(users_train)]\n",
    "                X_val = X[X['USER'].isin(users_val)]\n",
    "                X_test = X[X['USER'].isin(users_test)]\n",
    "                y_train = y[X['USER'].isin(users_train)]\n",
    "                y_val = y[X['USER'].isin(users_val)]\n",
    "                y_test = y[X['USER'].isin(users_test)]\n",
    "            \n",
    "            else:\n",
    "                X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=1-train_size, random_state=random_state)\n",
    "                X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=test_size, random_state=random_state)\n",
    "\n",
    "            num_ones = sum(y_test == 1)\n",
    "            num_zeros = sum(y_test == 0)\n",
    "            print(f\"Number of 1s in the test set: {num_ones}\")\n",
    "            print(f\"Number of 0s in the test set: {num_zeros}\")\n",
    "    \n",
    "            params = {\n",
    "                'iterations': 1000,\n",
    "                'eval_metric': 'AUC',\n",
    "                'cat_features': cat_features,\n",
    "                'early_stopping_rounds': 100,\n",
    "                'verbose': 50,\n",
    "                'random_seed': random_state\n",
    "            }\n",
    "\n",
    "            # Start the timer\n",
    "            start_time = time.time()\n",
    "    \n",
    "            # Initialize and train the model\n",
    "            cbc = CatBoostClassifier(**params)\n",
    "            cbc.fit(X_train, y_train,\n",
    "                    eval_set=(X_val, y_val),\n",
    "                    use_best_model=True,\n",
    "                    plot=False\n",
    "                   )\n",
    "\n",
    "            y_test_pred_proba = cbc.predict_proba(X_test)[:, 1] \n",
    "    \n",
    "            auc_score = roc_auc_score(y_test, y_test_pred_proba)\n",
    "            print(f'AUC Score Test-Data: {auc_score:.4f}')\n",
    "    \n",
    "            y_test_pred = cbc.predict(X_test)\n",
    "    \n",
    "            f1_model = f1_score(y_test, y_test_pred)\n",
    "            print(f'F1 Score Test-Data (Model): {f1_model:.4f}')\n",
    "    \n",
    "            conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "            print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "    \n",
    "            tn, fp, fn, tp = conf_matrix.ravel()\n",
    "    \n",
    "            feature_importance = dict(zip(X_train.columns, cbc.get_feature_importance()))\n",
    "    \n",
    "            # Stop the timer\n",
    "            end_time = time.time()\n",
    "            elapsed_time = end_time - start_time  # Calculate elapsed time in seconds\n",
    "            \n",
    "            # Append results for this run to the results list\n",
    "            results.append({\n",
    "                'Dataset_Name': dataset_name,\n",
    "                'Run_Number': run + 1,\n",
    "                'Random_State': random_state,\n",
    "                'Number_of_Data_Points': len(dataset),\n",
    "                'Number_of_1s_in_Test_Set': num_ones,\n",
    "                'Number_of_0s_in_Test_Set': num_zeros,\n",
    "                'AUC_Score': auc_score,\n",
    "                'F1_Score': f1_model,\n",
    "                'TP': tp,\n",
    "                'FP': fp,\n",
    "                'TN': tn,\n",
    "                'FN': fn,\n",
    "                'Feature_Importance': feature_importance,\n",
    "                'Elapsed_Time': elapsed_time \n",
    "            })\n",
    "            \n",
    "            # Save the split data with labels\n",
    "            save_split_data(X_train, X_val, X_test, y_train, y_val, y_test, y_test_pred, y_test_pred_proba, tn, fp, fn, tp, dataset_name, run)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv('Results_Usersplit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56121a13-5374-4fba-91da-8d19239f22bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_runs = 10  \n",
    "train_size = 0.6\n",
    "test_size = 0.5\n",
    "\n",
    "run_model(datasets_dict, num_runs, train_size, test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4425d66d-5983-4436-b29b-69cad0c00edf",
   "metadata": {},
   "source": [
    "## Calculate Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbdcded-e7ba-4148-86b6-6dca0fa11f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_delete_datasets(run):\n",
    "    UPA_I_file = f'UPA_I_run_{run+1}_Usersplit.csv'\n",
    "    UPA_T_file = f'UPA_T_run_{run+1}_Usersplit.csv'\n",
    "    UPA_R_file = f'UPA_R_run_{run+1}_Usersplit.csv'\n",
    "    \n",
    "    UPA_I_df = pd.read_csv(UPA_I_file)\n",
    "    UPA_T_df = pd.read_csv(UPA_T_file)\n",
    "    UPA_R_df = pd.read_csv(UPA_R_file)\n",
    "\n",
    "    merged_df = UPA_I_df[['USER', 'TCODE', 'Split', 'True_Target', 'Predicted_Target']].merge(\n",
    "        UPA_T_df[['USER', 'TCODE', 'Split', 'True_Target', 'Predicted_Target']],\n",
    "        on=['USER', 'TCODE'],\n",
    "        how='left',\n",
    "        suffixes=('_UPA_I', '_UPA_T')\n",
    "    ).merge(\n",
    "        UPA_R_df[['USER', 'TCODE', 'Split', 'True_Target', 'Predicted_Target']],\n",
    "        on=['USER', 'TCODE'],\n",
    "        how='left',\n",
    "        suffixes=('', '_UPA_R')\n",
    "    )\n",
    "\n",
    "    merged_df = merged_df.rename(columns={\n",
    "        'Split_UPA_I': 'Split_UPA_I',\n",
    "        'True_Target_UPA_I': 'True_Target_UPA_I',\n",
    "        'Predicted_Target_UPA_I': 'Predicted_Target_UPA_I',\n",
    "        'Split_UPA_T': 'Split_UPA_T',\n",
    "        'True_Target_UPA_T': 'True_Target_UPA_T',\n",
    "        'Predicted_Target_UPA_T': 'Predicted_Target_UPA_T',\n",
    "        'Split': 'Split_UPA_R',\n",
    "        'True_Target': 'True_Target_UPA_R',\n",
    "        'Predicted_Target': 'Predicted_Target_UPA_R'\n",
    "    })\n",
    "\n",
    "    file_name = f'Combined_dataset_run_{run+1}_Usersplit.csv'\n",
    "    merged_df.to_csv(file_name, index=False)\n",
    "    print(f'Combined dataset saved as {file_name}')\n",
    "\n",
    "    os.remove(UPA_I_file)\n",
    "    os.remove(UPA_T_file)\n",
    "    os.remove(UPA_R_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132feda1-79d9-452e-9874-4f97e73a9a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in range(0,num_runs):\n",
    "    merge_and_delete_datasets(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790e58f9-18cd-43af-b31e-ba2c25f3992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in range(1, num_runs + 1):\n",
    "    file_name = f\"Combined_dataset_run_{run}_Usersplit.csv\"\n",
    "    globals()[f\"Combined_dataset_run_{run}_Usersplit\"] = pd.read_csv(file_name, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f7f412-9d2b-411b-979e-8737f1f6dcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_info = [\n",
    "    {\"split_col\": \"Split_UPA_I\", \"true_col\": \"True_Target_UPA_I\", \"pred_col\": \"Predicted_Target_UPA_I\", \"name\": \"UPA_I\"},\n",
    "    {\"split_col\": \"Split_UPA_R\", \"true_col\": \"True_Target_UPA_R\", \"pred_col\": \"Predicted_Target_UPA_R\", \"name\": \"UPA_R\"},\n",
    "    {\"split_col\": \"Split_UPA_T\", \"true_col\": \"True_Target_UPA_T\", \"pred_col\": \"Predicted_Target_UPA_T\", \"name\": \"UPA_T\"},\n",
    "    {\"split_col\": \"Split_UPA_R\", \"true_col\": \"True_Target_UPA_R\", \"pred_col\": \"Predicted_Target_UPA_R\", \"name\": \"UPA_R_Assumption\", \"assumption\": True}\n",
    "]\n",
    "\n",
    "num_runs = 10\n",
    "f1_scores_over_runs = {dataset['name']: {\"f1_pos\": [], \"f1_neg\": [], \"f1_macro\": [], \"f1_weighted\": [], \"baseline_f1\": []} for dataset in datasets_info}\n",
    "\n",
    "for run in range(1, num_runs + 1):\n",
    "    combined_dataset_name = f\"Combined_dataset_run_{run}_Usersplit\"\n",
    "    combined_dataset = globals()[combined_dataset_name]\n",
    "    \n",
    "    print(f\"\\n===== Ergebnisse für Run {run} =====\")\n",
    "    \n",
    "    for dataset in datasets_info:\n",
    "        results = combined_dataset[[dataset['true_col'], dataset['pred_col']]].loc[combined_dataset[dataset['split_col']] == \"Test\"]\n",
    "        \n",
    "        if dataset.get(\"assumption\"):\n",
    "            results[dataset['true_col']] = results[dataset['true_col']].fillna(0.0)\n",
    "            results[dataset['pred_col']] = results[dataset['pred_col']].fillna(0.0)\n",
    "\n",
    "        class_distribution = results[dataset['true_col']].value_counts(normalize=True)\n",
    "        p_0 = class_distribution.get(0, 0) \n",
    "        p_1 = class_distribution.get(1, 0) \n",
    "        \n",
    "        baseline_predictions = np.random.choice([0, 1], size=len(results), p=[p_0, p_1])\n",
    "        baseline_f1 = f1_score(results[dataset['true_col']], baseline_predictions)\n",
    "\n",
    "        print(f\"Results for {dataset['name']} in Run {run}:\")\n",
    "        print(f\"Baseline F1 Score für {dataset['name']} in Run {run}: {baseline_f1:.4f}\")\n",
    "        \n",
    "        f1_pos = f1_score(results[dataset['true_col']], results[dataset['pred_col']], pos_label=1)\n",
    "        f1_neg = f1_score(results[dataset['true_col']], results[dataset['pred_col']], pos_label=0)\n",
    "        f1_macro = f1_score(results[dataset['true_col']], results[dataset['pred_col']], average=\"macro\")\n",
    "        f1_weighted = f1_score(results[dataset['true_col']], results[dataset['pred_col']], average=\"weighted\")\n",
    "        \n",
    "        cm = confusion_matrix(results[dataset['true_col']], results[dataset['pred_col']])\n",
    "        \n",
    "        print(\"Pos F1: \" + str(f1_pos))\n",
    "        print(\"Neg F1: \" + str(f1_neg))\n",
    "        print(\"Macro F1: \" + str(f1_macro))\n",
    "        print(\"Weighted F1: \" + str(f1_weighted))\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(cm)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        f1_scores_over_runs[dataset['name']]['baseline_f1'].append(baseline_f1)\n",
    "        f1_scores_over_runs[dataset['name']]['f1_pos'].append(f1_pos)\n",
    "        f1_scores_over_runs[dataset['name']]['f1_neg'].append(f1_neg)\n",
    "        f1_scores_over_runs[dataset['name']]['f1_macro'].append(f1_macro)\n",
    "        f1_scores_over_runs[dataset['name']]['f1_weighted'].append(f1_weighted)\n",
    "        f1_scores_over_runs[dataset['name']]['baseline_f1'].append(baseline_f1)\n",
    "\n",
    "print(\"\\n===== Avg and Std per Run =====\")\n",
    "for dataset in datasets_info:\n",
    "    name = dataset['name']\n",
    "    \n",
    "    avg_baseline_f1 = np.mean(f1_scores_over_runs[name]['baseline_f1'])\n",
    "    avg_f1_pos = np.mean(f1_scores_over_runs[name]['f1_pos'])\n",
    "    avg_f1_neg = np.mean(f1_scores_over_runs[name]['f1_neg'])\n",
    "    avg_f1_macro = np.mean(f1_scores_over_runs[name]['f1_macro'])\n",
    "    avg_f1_weighted = np.mean(f1_scores_over_runs[name]['f1_weighted'])\n",
    "    \n",
    "    std_baseline_f1 = np.std(f1_scores_over_runs[name]['baseline_f1'])\n",
    "    std_f1_pos = np.std(f1_scores_over_runs[name]['f1_pos'])\n",
    "    std_f1_neg = np.std(f1_scores_over_runs[name]['f1_neg'])\n",
    "    std_f1_macro = np.std(f1_scores_over_runs[name]['f1_macro'])\n",
    "    std_f1_weighted = np.std(f1_scores_over_runs[name]['f1_weighted'])\n",
    "    \n",
    "    print(f\"\\nAvg and Std for {name}:\")\n",
    "    print(f\"Avg Baseline F1: {avg_baseline_f1:.4f} (Std: {std_baseline_f1:.4f})\")\n",
    "    print(f\"Avg Pos F1: {avg_f1_pos:.4f} (Std: {std_f1_pos:.4f})\")\n",
    "    print(f\"Avg Neg F1: {avg_f1_neg:.4f} (Std: {std_f1_neg:.4f})\")\n",
    "    print(f\"Avg Macro F1: {avg_f1_macro:.4f} (Std: {std_f1_macro:.4f})\")\n",
    "    print(f\"Avg Weighted F1: {avg_f1_weighted:.4f} (Std: {std_f1_weighted:.4f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
